{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUBQgV9GqVyql5o3p3oTXH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akanshapawar21/akanshapawar/blob/main/MOD_01_LAB_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DypanfYXMV1h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "rng = np.random.default_rng(seed=42)\n",
        "from sklearn.utils.extmath import cartesian\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X = train_X[::1200,:,:].copy()\n",
        "train_y = train_y[::1200].copy()"
      ],
      "metadata": {
        "id": "L9Xe2MQJMgwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query\n",
        "  sq = diff*diff\n",
        "  dist = sq.sum(1)\n",
        "  label = trainlabel[np.argmin(dist)]\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "\n",
        "  traindata = traindata.reshape(-1, 28*28)\n",
        "  testdata = testdata.reshape(-1,28*28)\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum()\n",
        "  return correct/len(gtlabel)"
      ],
      "metadata": {
        "id": "VC_iTpcVMgsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testpred = NN(train_X, train_y, test_X)\n",
        "print('Baseline accuracy without augmentation is ', Accuracy(test_y, testpred))"
      ],
      "metadata": {
        "id": "uiGJ5nqlMqRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_X[2], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(rotate(train_X[2],27), cmap='gray')"
      ],
      "metadata": {
        "id": "GdKOXCcsMgqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augRotate(sample, angleconstraint):\n",
        "  if angleconstraint==0:\n",
        "    return sample\n",
        "  if len(sample.shape)==2:\n",
        "    sample = np.expand_dims(sample, 0)  # make sure the sample is 3 dimensional\n",
        "  angle = rng.random(len(sample)) # generate random numbers for angles\n",
        "  angle = (angle-0.5)*angleconstraint # make the random angle constrained\n",
        "  nsample = sample.copy() # preallocate the augmented array to make it faster\n",
        "  for ii in range(len(sample)):\n",
        "    nsample[ii] = rotate(sample[ii], angle[ii])\n",
        "  return np.squeeze(nsample) # take care if the input had only one sample."
      ],
      "metadata": {
        "id": "QBiXA2r5Mgn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_X[20]\n",
        "angleconstraint = 70\n",
        "# show the original image\n",
        "plt.imshow(sample, cmap='gray')\n",
        "plt.show()\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(augRotate(sample, angleconstraint), cmap='gray') # show an augmented image\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(augRotate(sample, angleconstraint), cmap='gray') # show another augmented image from the same sample\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(augRotate(sample, angleconstraint), cmap='gray') # one more image from the same sample"
      ],
      "metadata": {
        "id": "mO0x0GL6Mgli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "angleconstraint = 60\n",
        "naugmentations = 5\n",
        "\n",
        "# augment\n",
        "augdata = train_X # we include the original images also in the augmented dataset\n",
        "auglabel = train_y\n",
        "for ii in range(naugmentations):\n",
        "  augdata = np.concatenate((augdata, augRotate(train_X, angleconstraint))) # concatenate the augmented data to the set\n",
        "  auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "# check the test accuracy\n",
        "testpred = NN(augdata, auglabel, test_X)\n",
        "print('Accuracy after rotation augmentation is ', Accuracy(test_y, testpred))"
      ],
      "metadata": {
        "id": "7Ca27l9SMgjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angleconstraints = [0,10,20,30,40,50,60,70,80,90] # the values we want to test\n",
        "accuracies = np.zeros(len(angleconstraints), dtype=np.float) # we will save the values here\n",
        "\n",
        "for ii in range(len(angleconstraints)):\n",
        "  # create the augmented dataset\n",
        "  augdata = train_X # we include the original images also in the augmented dataset\n",
        "  auglabel = train_y\n",
        "  for jj in range(naugmentations):\n",
        "    augdata = np.concatenate((augdata, augRotate(train_X, angleconstraints[ii]))) # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "  # check the test accuracy\n",
        "  testpred = NN(augdata, auglabel, test_X)\n",
        "  accuracies[ii] = Accuracy(test_y, testpred)\n",
        "  print('Accuracy after rotation augmentation constrained by ',angleconstraints[ii], ' is ', accuracies[ii], flush=True)"
      ],
      "metadata": {
        "id": "UFFYumkqM3M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "# plot the variation of accuracy\n",
        "ax.plot(angleconstraints, accuracies)\n",
        "ax.set_xlabel('angle')\n",
        "ax.set_ylabel('accuracy')\n",
        "# plot the maximum accuracy\n",
        "maxind = np.argmax(accuracies)\n",
        "plt.scatter(angleconstraints[maxind], accuracies[maxind], c='red')"
      ],
      "metadata": {
        "id": "ulSiVyCDM3Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shear(sample, amount):\n",
        "  tform = AffineTransform(shear = amount) # create the shear transform\n",
        "  img = warp(sample, tform) # apply the shear\n",
        "  # this makes the digit off-center. Since all the images in the test set are centralized, we will do the same here\n",
        "  col = img.sum(0).nonzero()[0]\n",
        "  row = img.sum(1).nonzero()[0]\n",
        "  if len(col)>0 and len(row)>0:\n",
        "    xshift = int(sample.shape[0]/2 - (row[0]+row[-1])/2)\n",
        "    yshift = int(sample.shape[1]/2 - (col[0]+col[-1])/2)\n",
        "    img = np.roll(img, (xshift, yshift),(0,1))\n",
        "  return img"
      ],
      "metadata": {
        "id": "fLAjs0fIM3Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_X[2]\n",
        "plt.imshow(sample, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# apply shear\n",
        "plt.imshow(shear(sample, 0.4), cmap='gray')"
      ],
      "metadata": {
        "id": "jSwPb7kQM3Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augShear(sample, shearconstraint):\n",
        "  if shearconstraint==0:\n",
        "    return sample\n",
        "  if len(sample.shape)==2:\n",
        "    sample = np.expand_dims(sample, 0)  # make sure the sample is 3 dimensional\n",
        "  amt = rng.random(len(sample)) # generate random numbers for shear\n",
        "  amt = (amt-0.5)*shearconstraint # make the random shear constrained\n",
        "  nsample = sample.copy() # preallocate the augmented array to make it faster\n",
        "  for ii in range(len(sample)):\n",
        "    nsample[ii] = shear(sample[ii], amt[ii])\n",
        "  return np.squeeze(nsample) # take care if the input had only one sample."
      ],
      "metadata": {
        "id": "YgMr9Py7NLVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shearconstraints = [0, 0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0] # the values we want to test\n",
        "accuracies = np.zeros(len(shearconstraints), dtype=np.float) # we will save the values here\n",
        "\n",
        "for ii in range(len(shearconstraints)):\n",
        "  # create the augmented dataset\n",
        "  augdata = train_X # we include the original images also in the augmented dataset\n",
        "  auglabel = train_y\n",
        "  for jj in range(naugmentations):\n",
        "    augdata = np.concatenate((augdata, augShear(train_X, shearconstraints[ii]))) # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "  # check the test accuracy\n",
        "  testpred = NN(augdata, auglabel, test_X)\n",
        "  accuracies[ii] = Accuracy(test_y, testpred)\n",
        "  print('Accuracy after shear augmentation constrained by ',shearconstraints[ii], ' is ', accuracies[ii], flush=True)"
      ],
      "metadata": {
        "id": "H3_B1c4eNLSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "# plot the variation of accuracy\n",
        "ax.plot(shearconstraints, accuracies)\n",
        "ax.set_xlabel('angle')\n",
        "ax.set_ylabel('accuracy')\n",
        "# plot the maximum accuracy\n",
        "maxind = np.argmax(accuracies)\n",
        "plt.scatter(shearconstraints[maxind], accuracies[maxind], c='red')"
      ],
      "metadata": {
        "id": "55mpj118NLPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augRotateShear(sample, angleconstraint, shearconstraint):\n",
        "  if len(sample.shape)==2:\n",
        "    sample = np.expand_dims(sample, 0)  # make sure the sample is 3 dimensional\n",
        "  amt = rng.random(len(sample)) # generate random numbers for shear\n",
        "  amt = (amt-0.5)*shearconstraint # make the random shear constrained\n",
        "  angle = rng.random(len(sample)) # generate random numbers for angles\n",
        "  angle = (angle-0.5)*angleconstraint # make the random angle constrained\n",
        "  nsample = sample.copy() # preallocate the augmented array to make it faster\n",
        "  for ii in range(len(sample)):\n",
        "    nsample[ii] = rotate(shear(sample[ii], amt[ii]), angle[ii]) # first apply shear, then rotate\n",
        "  return np.squeeze(nsample) # take care if the input had only one sample."
      ],
      "metadata": {
        "id": "tXve30XBNYad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shearconstraints = [0, 0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6] # the values we want to test\n",
        "angleconstraints = [0,10,20,30,40,50,60] # the values we want to test\n",
        "hyp = cartesian((shearconstraints, angleconstraints)) # cartesian product of both\n",
        "\n",
        "accuracies = np.zeros(len(hyp), dtype=np.float) # we will save the values here\n",
        "\n",
        "for ii in range(len(hyp)):\n",
        "  # create the augmented dataset\n",
        "  augdata = train_X # we include the original images also in the augmented dataset\n",
        "  auglabel = train_y\n",
        "  for jj in range(naugmentations):\n",
        "    augdata = np.concatenate((augdata, augRotateShear(train_X, hyp[ii][0], hyp[ii][1]))) # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate((auglabel, train_y))  # the labels don't change when we augment\n",
        "\n",
        "  # check the test accuracy\n",
        "  testpred = NN(augdata, auglabel, test_X)\n",
        "  accuracies[ii] = Accuracy(test_y, testpred)\n",
        "  print('Accuracy after augmentation shear:',hyp[ii][0], 'angle:',hyp[ii][1], ' is ', accuracies[ii], flush=True)"
      ],
      "metadata": {
        "id": "GomFFlF1NYOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.9, 0.9]) # main axes\n",
        "im = ax.imshow(accuracies.reshape((len(shearconstraints), len(angleconstraints))), cmap='inferno')\n",
        "ax.set_xlabel('angle')\n",
        "ax.set_ylabel('shear')\n",
        "ax.set_xticks(np.arange(len(angleconstraints)));\n",
        "ax.set_xticklabels(angleconstraints);\n",
        "ax.set_yticks(np.arange(len(shearconstraints)));\n",
        "ax.set_yticklabels(shearconstraints);\n",
        "plt.colorbar(im)"
      ],
      "metadata": {
        "id": "WiY3E127NdiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Questions\n",
        "Try these questions for better understanding. You may not be able to solve all of them.\n",
        "\n",
        "What is the best value for angle constraint and shear constraint you got? How much did the accuracy improve as compared to not using augmentations?\n",
        "ANS: Best Shear Constraint: Look for the shear constraint value that corresponds to the highest accuracy in the accuracies array. The value of shearconstraints[ii] at the maximum accuracy should give you the best shear constraint.\n",
        "\n",
        "Accuracy Improvement: To measure the accuracy improvement compared to not using augmentations, you can compare the accuracy achieved with the best shear constraint to the accuracy obtained without any augmentations.\n",
        "\n",
        "Can you increase the accuracy by increasing the number of augmentations from each sample?\n",
        "ANS: Yes, increasing the number of augmentations from each sample can potentially improve model accuracy by providing more diverse training data, helping the model generalize better to different inputs and reducing overfitting\n",
        "\n",
        "Try implementing a few augmentations of your own and experimenting with them. A good reference is here.\n",
        "Try combining various augmentations. What is the highest accuracy you can get? What is the smallest training dataset you can take and still get accuracy above 50%?\n",
        "ANS:\n",
        "The highest achievable accuracy by combining various augmentations depends on your specific dataset and task. There's no fixed value, and it varies.\n",
        "\n",
        "Whenever you do any experiment, a good practice is to vary the hyperparameters gradually and create a graph of your results, like we did for gridsearch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zoCAWcAANh7m"
      }
    }
  ]
}